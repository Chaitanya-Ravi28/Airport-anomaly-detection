{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053376e8-b8c5-46a1-a5a3-81d07daea97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Ledger saved â†’ /home/jovyan/security_ledger.json (1930 blocks)\n",
      "âœ… Ledger verified: 1930 blocks OK.\n",
      "\n",
      "ðŸ”Ž Preview first 12 blocks:\n",
      "   1 | video      | Crowd Movement     | score: 0.004062 | time: 2025-11-12 08:04:10.977407\n",
      "   2 | video      | Crowd Movement     | score: 0.004302 | time: 2025-11-12 08:04:10.977434\n",
      "   3 | video      | Crowd Movement     | score: 0.004546 | time: 2025-11-12 08:04:10.977437\n",
      "   4 | video      | Crowd Movement     | score: 0.004793 | time: 2025-11-12 08:04:10.977439\n",
      "   5 | video      | Crowd Movement     | score: 0.004971 | time: 2025-11-12 08:04:10.977442\n",
      "   6 | video      | Crowd Movement     | score: 0.005081 | time: 2025-11-12 08:04:10.977444\n",
      "   7 | video      | Crowd Movement     | score: 0.005124 | time: 2025-11-12 08:04:10.977447\n",
      "   8 | video      | Crowd Movement     | score: 0.005176 | time: 2025-11-12 08:04:10.977451\n",
      "   9 | video      | Crowd Movement     | score: 0.005243 | time: 2025-11-12 08:04:10.977454\n",
      "  10 | video      | Crowd Movement     | score: 0.005244 | time: 2025-11-12 08:04:10.977457\n",
      "  11 | video      | Crowd Movement     | score: 0.005276 | time: 2025-11-12 08:04:10.977459\n",
      "  12 | video      | Crowd Movement     | score: 0.005248 | time: 2025-11-12 08:04:10.977461\n",
      "\n",
      "ðŸ“Œ Counts per source: {'video': 306, 'iot': 32, 'prohibited': 1592}\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Blockchain builder \n",
    "# ---------------------------\n",
    "import json, hashlib, uuid, os, tempfile\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "HOME = Path(\"/home/jovyan\")\n",
    "\n",
    "# Paths \n",
    "FILES = {\n",
    "    \"video\": HOME / \"Video_analysis+IOT\" / \"outputs\" / \"video_anomalies.json\",\n",
    "    \"iot\": HOME / \"Video_analysis+IOT\" / \"outputs\" / \"iot_anomalies.json\",\n",
    "    \"prohibited\": HOME / \"CLCXray_yolo\" / \"outputs\" / \"prohibited_items.json\",\n",
    "}\n",
    "\n",
    "LEDGER_PATH = HOME / \"security_ledger.json\"\n",
    "\n",
    "#  Helpers \n",
    "def sha256_hex(s: str) -> str:\n",
    "    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def atomic_write(path: Path, data: str) -> None:\n",
    "    d = path.parent\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "    fd, tmp = tempfile.mkstemp(prefix=path.name, dir=str(d))\n",
    "    try:\n",
    "        with os.fdopen(fd, \"w\") as f:\n",
    "            f.write(data)\n",
    "        os.replace(tmp, str(path))\n",
    "    finally:\n",
    "        if os.path.exists(tmp):\n",
    "            try: os.remove(tmp)\n",
    "            except: pass\n",
    "\n",
    "def load_json_file(p: Path) -> List[Dict[str,Any]]:\n",
    "    if not p.exists():\n",
    "        # no file is okay â€” return empty\n",
    "        return []\n",
    "    try:\n",
    "        with open(p, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Failed to read {p}: {e}\")\n",
    "        return []\n",
    "    # normalize shapes\n",
    "    if isinstance(data, list):\n",
    "        return data\n",
    "    if isinstance(data, dict):\n",
    "        # common patterns: {\"detections\": [...] } or dict of filename->list\n",
    "        if \"detections\" in data and isinstance(data[\"detections\"], list):\n",
    "            return data[\"detections\"]\n",
    "        # if dict of lists, flatten\n",
    "        flattened = []\n",
    "        for v in data.values():\n",
    "            if isinstance(v, list):\n",
    "                flattened.extend(v)\n",
    "        if flattened:\n",
    "            return flattened\n",
    "        # otherwise wrap single dict as list\n",
    "        return [data]\n",
    "    return []\n",
    "\n",
    "def extract_field(entry: Dict[str,Any], keys):\n",
    "    for k in keys:\n",
    "        if k in entry and entry[k] is not None:\n",
    "            return entry[k]\n",
    "    return None\n",
    "\n",
    "def normalize_event(entry: Dict[str,Any], source: str) -> Dict[str,Any]:\n",
    "    # Build a normalized event dict for the ledger\n",
    "    # event_id: deterministic if possible (filename+timestamp+class) else UUID\n",
    "    filename = extract_field(entry, [\"filename\",\"file\",\"image\",\"img\",\"path\"])\n",
    "    ts = extract_field(entry, [\"timestamp\",\"time\",\"datetime\"]) or (datetime.utcnow().isoformat()+\"Z\")\n",
    "    item_class = extract_field(entry, [\"item_class\",\"class\",\"label\",\"anomaly_type\",\"type\",\"name\"]) or \"unknown\"\n",
    "    # score priority: anomaly_score -> confidence -> score -> recon_error\n",
    "    score = extract_field(entry, [\"anomaly_score\",\"confidence\",\"score\",\"recon_error\"])\n",
    "    try:\n",
    "        score = float(score) if score is not None else 0.0\n",
    "    except:\n",
    "        score = 0.0\n",
    "    # details: keep small useful keys\n",
    "    details = {}\n",
    "    for k in [\"details\",\"index\",\"frame\",\"bbox\",\"annotations\",\"info\"]:\n",
    "        if k in entry:\n",
    "            details[k] = entry[k]\n",
    "    if filename: details[\"filename\"] = filename\n",
    "\n",
    "    # create deterministic event id when possible\n",
    "    if filename or ts or item_class:\n",
    "        fingerprint = f\"{filename}|{ts}|{item_class}\"\n",
    "        event_id = sha256_hex(fingerprint)[:32]\n",
    "    else:\n",
    "        event_id = str(uuid.uuid4())\n",
    "\n",
    "    return {\n",
    "        \"event_id\": event_id,\n",
    "        \"timestamp\": ts,\n",
    "        \"source\": source,\n",
    "        \"item_class\": str(item_class),\n",
    "        \"anomaly_score\": float(round(score, 6)),\n",
    "        \"details\": details\n",
    "    }\n",
    "\n",
    "# Collect events from all files \n",
    "def collect_all_events(paths_map=FILES) -> List[Dict[str,Any]]:\n",
    "    events = []\n",
    "    for src, pth in paths_map.items():\n",
    "        p = Path(pth)\n",
    "        data = load_json_file(p)\n",
    "        if not data:\n",
    "            # print(f\"âš ï¸ No data found for {src} at {p}\")\n",
    "            continue\n",
    "        for entry in data:\n",
    "            ev = normalize_event(entry, src)\n",
    "            events.append(ev)\n",
    "    # sort by timestamp for deterministic ordering\n",
    "    try:\n",
    "        events.sort(key=lambda x: x[\"timestamp\"])\n",
    "    except Exception:\n",
    "        pass\n",
    "    return events\n",
    "\n",
    "#  Build chain from events list \n",
    "def build_chain(events: List[Dict[str,Any]]) -> List[Dict[str,Any]]:\n",
    "    chain = []\n",
    "    prev_hash = \"0\"\n",
    "    for idx, ev in enumerate(events, start=1):\n",
    "        block = {\n",
    "            \"index\": idx,\n",
    "            \"event_id\": ev[\"event_id\"],\n",
    "            \"timestamp\": ev[\"timestamp\"],\n",
    "            \"source\": ev[\"source\"],\n",
    "            \"item_class\": ev.get(\"item_class\",\"unknown\"),\n",
    "            \"anomaly_score\": ev.get(\"anomaly_score\", 0.0),\n",
    "            \"details\": ev.get(\"details\", {}),\n",
    "            \"prev_hash\": prev_hash\n",
    "        }\n",
    "        # compute hash over sorted keys (without hash)\n",
    "        block_str = json.dumps(block, sort_keys=True)\n",
    "        block_hash = sha256_hex(block_str)\n",
    "        block[\"hash\"] = block_hash\n",
    "        chain.append(block)\n",
    "        prev_hash = block_hash\n",
    "    return chain\n",
    "\n",
    "# Save ledger atomically \n",
    "def save_ledger(chain: List[Dict[str,Any]], path: Path = LEDGER_PATH) -> None:\n",
    "    atomic_write(path, json.dumps(chain, indent=4))\n",
    "    print(f\"âœ… Ledger saved â†’ {path} ({len(chain)} blocks)\")\n",
    "\n",
    "# Verify ledger integrity \n",
    "def verify_ledger(path: Path = LEDGER_PATH) -> bool:\n",
    "    if not path.exists():\n",
    "        print(\"âŒ Ledger file not found.\")\n",
    "        return False\n",
    "    try:\n",
    "        with open(path, \"r\") as f:\n",
    "            chain = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(\"âŒ Failed to read ledger:\", e)\n",
    "        return False\n",
    "    prev_hash = \"0\"\n",
    "    for b in chain:\n",
    "        copy_block = {k: b[k] for k in b if k != \"hash\"}\n",
    "        recalculated = sha256_hex(json.dumps(copy_block, sort_keys=True))\n",
    "        if recalculated != b.get(\"hash\"):\n",
    "            print(f\"âŒ Hash mismatch at index {b.get('index')}\")\n",
    "            return False\n",
    "        if b.get(\"prev_hash\") != prev_hash:\n",
    "            print(f\"âŒ prev_hash mismatch at index {b.get('index')}\")\n",
    "            return False\n",
    "        prev_hash = b.get(\"hash\")\n",
    "    print(f\"âœ… Ledger verified: {len(chain)} blocks OK.\")\n",
    "    return True\n",
    "\n",
    "# Build & save ledger (rebuild mode overwrites) \n",
    "def build_and_save_ledger(rebuild: bool = True) -> List[Dict[str,Any]]:\n",
    "    events = collect_all_events(FILES)\n",
    "    if not events:\n",
    "        print(\"âš ï¸ No events found in input files. Ledger not updated.\")\n",
    "        return []\n",
    "    # build chain anew (rebuild) or append to existing\n",
    "    if rebuild or not LEDGER_PATH.exists():\n",
    "        chain = build_chain(events)\n",
    "        save_ledger(chain, LEDGER_PATH)\n",
    "    else:\n",
    "        # append mode: read existing and append only new event_ids\n",
    "        with open(LEDGER_PATH, \"r\") as f:\n",
    "            existing = json.load(f)\n",
    "        existing_ids = {b.get(\"event_id\") for b in existing}\n",
    "        new_events = [e for e in events if e[\"event_id\"] not in existing_ids]\n",
    "        if not new_events:\n",
    "            print(\"âœ… No new events to append.\")\n",
    "            return existing\n",
    "        start_idx = len(existing) + 1\n",
    "        prev_hash = existing[-1][\"hash\"] if existing else \"0\"\n",
    "        new_blocks = []\n",
    "        for i, ev in enumerate(new_events):\n",
    "            block = {\n",
    "                \"index\": start_idx + i,\n",
    "                \"event_id\": ev[\"event_id\"],\n",
    "                \"timestamp\": ev[\"timestamp\"],\n",
    "                \"source\": ev[\"source\"],\n",
    "                \"item_class\": ev.get(\"item_class\",\"unknown\"),\n",
    "                \"anomaly_score\": ev.get(\"anomaly_score\",0.0),\n",
    "                \"details\": ev.get(\"details\", {}),\n",
    "                \"prev_hash\": prev_hash\n",
    "            }\n",
    "            block[\"hash\"] = sha256_hex(json.dumps(block, sort_keys=True))\n",
    "            new_blocks.append(block)\n",
    "            prev_hash = block[\"hash\"]\n",
    "        full_chain = existing + new_blocks\n",
    "        save_ledger(full_chain, LEDGER_PATH)\n",
    "        chain = full_chain\n",
    "    return chain\n",
    "\n",
    "# Quick preview helper \n",
    "def preview_ledger(n=10, path: Path = LEDGER_PATH):\n",
    "    if not path.exists():\n",
    "        print(\"âŒ Ledger not found.\")\n",
    "        return\n",
    "    with open(path, \"r\") as f:\n",
    "        chain = json.load(f)\n",
    "    for b in chain[:n]:\n",
    "        print(f\"{b['index']:>4} | {b['source']:<10} | {b['item_class']:<18} | score: {b['anomaly_score']:<6} | time: {b['timestamp']}\")\n",
    "\n",
    "# ---------------------------\n",
    "# RUN: build ledger now (rebuild -> overwrite)\n",
    "# ---------------------------\n",
    "chain = build_and_save_ledger(rebuild=True)\n",
    "\n",
    "# verify and preview\n",
    "if chain:\n",
    "    verify_ledger()\n",
    "    print(\"\\nðŸ”Ž Preview first 12 blocks:\")\n",
    "    preview_ledger(12)\n",
    "    # print counts per source\n",
    "    from collections import Counter\n",
    "    src_counts = Counter([b[\"source\"] for b in chain])\n",
    "    print(\"\\nðŸ“Œ Counts per source:\", dict(src_counts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbc6ec2-b533-4e8c-9706-43e01f765342",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
